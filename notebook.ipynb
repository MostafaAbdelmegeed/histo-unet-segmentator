{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# PART A: DATA PREPARATION (ROI + GeoJSON)\n",
    "# ---------------------------------------------\n",
    "def create_mask_from_geojson(roi_tif_path, geojson_path):\n",
    "    \"\"\"\n",
    "    1) Loads the ROI tif image using rasterio\n",
    "    2) Clips the polygons in the GeoJSON to ROI bounds\n",
    "    3) Rasterizes them to produce a mask (0=background, 1=object)\n",
    "    Returns:\n",
    "        roi_img (np.ndarray), mask (np.ndarray), metadata (dict)\n",
    "    \"\"\"\n",
    "    # -- Load ROI TIF --\n",
    "    with rasterio.open(roi_tif_path) as roi_src:\n",
    "        roi_img = roi_src.read(1)  # read first band; adapt if multi-band\n",
    "        roi_transform = roi_src.transform\n",
    "        roi_crs = roi_src.crs\n",
    "        roi_width = roi_src.width\n",
    "        roi_height = roi_src.height\n",
    "        roi_bounds = roi_src.bounds\n",
    "\n",
    "    # -- Read GeoJSON as GeoDataFrame --\n",
    "    geo_df = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # -- Clip polygons to ROI bounding box --\n",
    "    roi_polygon = geom.box(*roi_bounds)\n",
    "    geo_df_clipped = gpd.clip(geo_df, roi_polygon)\n",
    "\n",
    "    # -- Rasterize polygons to a mask --\n",
    "    shapes_to_rasterize = [(geom, 1) for geom in geo_df_clipped.geometry if geom is not None]\n",
    "    mask = features.rasterize(\n",
    "        shapes=shapes_to_rasterize,\n",
    "        out_shape=(roi_height, roi_width),\n",
    "        transform=roi_transform,\n",
    "        fill=0,\n",
    "        all_touched=True\n",
    "    )\n",
    "\n",
    "    # -- Prepare metadata to help with debugging or saving --\n",
    "    metadata = {\n",
    "        'transform': roi_transform,\n",
    "        'crs': roi_crs,\n",
    "        'width': roi_width,\n",
    "        'height': roi_height\n",
    "    }\n",
    "    return roi_img, mask, metadata\n",
    "\n",
    "\n",
    "class ROISegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal PyTorch Dataset for ROI segmentation.\n",
    "    Expects pre-loaded images and masks (or can load on-the-fly if needed).\n",
    "    \"\"\"\n",
    "    def __init__(self, roi_img, mask, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            roi_img (np.ndarray): The ROI image, shape (H, W)\n",
    "            mask (np.ndarray): The segmentation mask, shape (H, W)\n",
    "            transform (callable, optional): Optional transform for data augmentation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.roi_img = roi_img\n",
    "        self.mask = mask\n",
    "        self.transform = transform\n",
    "\n",
    "        # For demonstration, we'll store patches or just the entire image\n",
    "        # as a single sample. Typically you'd want to tile large images.\n",
    "        # Here, let's just keep it as 1 sample for simplicity.\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # only one image/mask pair in this minimal example\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.roi_img\n",
    "        label = self.mask\n",
    "\n",
    "        # Convert to float32\n",
    "        image = image.astype(np.float32)\n",
    "        label = label.astype(np.float32)\n",
    "\n",
    "        # Expand dimension to create 1 channel: (1, H, W)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        label = np.expand_dims(label, axis=0)  # still binary but keep shape consistent\n",
    "\n",
    "        if self.transform:\n",
    "            # If we have torchvision-based transforms that work on PIL images,\n",
    "            # we might need to convert from numpy to PIL, apply transform,\n",
    "            # then convert back to tensor. Or use custom transforms on numpy arrays.\n",
    "            pass\n",
    "\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ALTERNATIVE: Using segmentation_models_pytorch\\n# import segmentation_models_pytorch as smp\\n# model = smp.Unet(\\n#     encoder_name=\"resnet34\",  # or any other encoder\\n#     encoder_weights=\"imagenet\",\\n#     in_channels=1,\\n#     classes=1,\\n# )\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# PART B: DEFINE A SIMPLE U-NET\n",
    "# ---------------------------------------------\n",
    "# Option 1: Implement a minimal U-Net from scratch (short version)\n",
    "# Option 2: Use segmentation_models_pytorch (commented below)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(SimpleUNet, self).__init__()\n",
    "        # Down\n",
    "        self.dc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.dc3 = DoubleConv(128, 256)\n",
    "\n",
    "        # Up\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dc4 = DoubleConv(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dc5 = DoubleConv(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Down\n",
    "        x1 = self.dc1(x)\n",
    "        x2 = self.pool1(x1)\n",
    "        x3 = self.dc2(x2)\n",
    "        x4 = self.pool2(x3)\n",
    "\n",
    "        # Bottleneck\n",
    "        x5 = self.dc3(x4)\n",
    "\n",
    "        # Up\n",
    "        x6 = self.up1(x5)\n",
    "        # Concat skip\n",
    "        x6 = torch.cat([x6, x3], dim=1)\n",
    "        x7 = self.dc4(x6)\n",
    "        x8 = self.up2(x7)\n",
    "        x8 = torch.cat([x8, x1], dim=1)\n",
    "        x9 = self.dc5(x8)\n",
    "\n",
    "        out = self.out_conv(x9)\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "# ALTERNATIVE: Using segmentation_models_pytorch\n",
    "# import segmentation_models_pytorch as smp\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",  # or any other encoder\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=1,\n",
    "#     classes=1,\n",
    "# )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# PART C: TRAINING AND VALIDATION LOOP\n",
    "# ---------------------------------------------\n",
    "def dice_coefficient(pred, target, smooth=1.0):\n",
    "    \"\"\"\n",
    "    Calculates the Dice coefficient for binary segmentation.\n",
    "    pred, target: expected shape (N, 1, H, W)\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)  # convert logits to [0..1]\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    return (2.0 * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for images, masks in loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            dice = dice_coefficient(outputs, masks)\n",
    "            dice_scores.append(dice.item())\n",
    "    return np.mean(dice_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mosta\\Workspace\\repos\\histo\\.venv\\Lib\\site-packages\\rasterio\\__init__.py:368: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.4834, Val Dice: 0.9212\n",
      "Epoch [2/10] - Loss: 0.1630, Val Dice: 0.9632\n",
      "Epoch [3/10] - Loss: 0.0768, Val Dice: 0.9784\n",
      "Epoch [4/10] - Loss: 0.0451, Val Dice: 0.9860\n",
      "Epoch [5/10] - Loss: 0.0292, Val Dice: 0.9906\n",
      "Epoch [6/10] - Loss: 0.0195, Val Dice: 0.9936\n",
      "Epoch [7/10] - Loss: 0.0133, Val Dice: 0.9956\n",
      "Epoch [8/10] - Loss: 0.0091, Val Dice: 0.9970\n",
      "Epoch [9/10] - Loss: 0.0062, Val Dice: 0.9979\n",
      "Epoch [10/10] - Loss: 0.0042, Val Dice: 0.9986\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# PART D: MAIN SCRIPT TO RUN EVERYTHING\n",
    "# ---------------------------------------------\n",
    "def main():\n",
    "    # 1. File paths\n",
    "    roi_tif_path = \"data/roi.tif\"\n",
    "    geojson_path = \"data/labels.geojson\"\n",
    "\n",
    "    # 2. Create ROI image + mask\n",
    "    roi_img, roi_mask, metadata = create_mask_from_geojson(roi_tif_path, geojson_path)\n",
    "\n",
    "    # 3. Create Dataset\n",
    "    dataset = ROISegDataset(roi_img=roi_img, mask=roi_mask)\n",
    "    # For demonstration, weâ€™ll use the same dataset for train & val\n",
    "    # But in real practice, you should have a separate hold-out set.\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # 4. Initialize Model, Criterion, Optimizer\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SimpleUNet(in_channels=1, out_channels=1).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # 5. Train\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, loader, optimizer, criterion, device)\n",
    "        val_dice = validate_one_epoch(model, loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {train_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "    # 6. Save the trained model\n",
    "    torch.save(model.state_dict(), \"simple_unet_roi_seg.pth\")\n",
    "\n",
    "    # Optionally, reload and do inference later\n",
    "    # model.load_state_dict(torch.load(\"simple_unet_roi_seg.pth\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
